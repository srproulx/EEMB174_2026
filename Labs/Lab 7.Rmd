---
title: "Lab 7"
author: "Stephen R. Proulx"
output: pdf_document
---

```{r  setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(rethinking)
library(bayesplot)
source("../helper.R")
options(rethinking_cmdstan = TRUE)
```
# Bayesian Statistical Modeling Winter 2026
# Lab Exercise, Week 8


*When is this lab due?* Labs are due on the Thursday after they are assigned. However, in many cases you can complete them during the lab period itself.   This assignment is due on Thursday, 3/5/2026.  

## Load and process the data


Load the dataset. It contains Covid case numbers and deaths for all counties in California.
```{r }
load("SB_covid_data_2022.Rdata")
```


We're going to calculate some new columns that summarize weekly total Covid cases. To do this we need to group by the county, and make sure the data are in order by date. The column `delta.days` is the number of days elapsed since data on COVID cases were first reported last winter. 

```{r   }
county_ca <- group_by(county_ca,county) %>%
  arrange(delta.days) %>%
  mutate(new_cases = cases-lag(cases), 
         week_cases=(cases-lag(cases,7))/7, 
         week_old_cases=(lag(cases,7)-lag(cases,14))/7, 
         two_week_old_cases=(lag(cases,14)-lag(cases,21))/7,
         three_week_old_cases=(lag(cases,21)-lag(cases,28))/7,
         four_week_old_cases=(lag(cases,28)-lag(cases,35))/7,
         new_deaths = deaths-lag(deaths)) %>%
  ungroup()
```


prune down to first 500 days
```{r}
county_ca<-filter(county_ca,delta.days<500  )
```


We are now going to pull out the Santa Barbara county data, but only since day 100.  
```{r  }
firstday=101
  
  sb_data<-filter(county_ca,delta.days>firstday,county == "Santa Barbara", state=="California")

```


## Check out the data

This draws a cubic spline fit through the 7-day average case number.
```{r  , warning=FALSE}
firstday=101
maxday=500
ssvals <- smooth.spline(x=sb_data$delta.days, y= sb_data$week_cases, df=15)
spline_data <- tibble( delta.days=ssvals$x, week_cases = ssvals$y)
 

ggplot(data=sb_data, aes(x=delta.days,y= week_cases/7)) +
  geom_point()+
  geom_line(data=spline_data,color="red")+
  scale_y_continuous( limits=c(1,100) )+
  scale_x_continuous(limits=c(firstday,maxday))+
  labs( x="days since Jan 22" , y="7-day average cases")


```



```{r }
ggplot(data=sb_data, aes(x=delta.days,y=new_deaths )) +
  geom_point()+
  scale_y_continuous( limits=c(-2,20) )+
  scale_x_continuous(limits=c(firstday,maxday))+
  labs( x="days since Jan 22" , y="deaths")

```

Now we'll prune down some of the columns we are keeping and remove any negative numbers (these are due to corrections in the reporting after the fact and sometimes make it appear that people were un-dead)
```{r}
stan_data <- select(sb_data,new_cases, week_cases ,week_old_cases, two_week_old_cases, three_week_old_cases, four_week_old_cases, new_deaths) %>%
  mutate(new_deaths= new_deaths * (new_deaths>0) ) %>%
  mutate(NC=standardize(new_cases),
         W0=standardize(week_cases),
         W1=standardize(week_old_cases),
         W2=standardize(two_week_old_cases),
         W3=standardize(three_week_old_cases),
         W4=standardize(four_week_old_cases))%>%
  rowid_to_column(var="index") 
```


## Example: Fitting the overall mortality with a Poisson 
```{r }
m.poiss.overall <- ulam(alist(
   new_deaths ~ dpois(lambda),
   log(lambda) <- log_lambda,
   log_lambda ~ dnorm(0,2)
),data=stan_data ,
  iter = 4000,chains=4, log_lik = TRUE,seed=42)
```

```{r }
precis(m.poiss.overall,digits=4)
```

The Rhat is close enough to 1, and no warnings were reported. 

We can check the WAIC. The raw value does not have meaning, but we can later compare it to other models.
```{r }
WAIC(m.poiss.overall)
```


How does it compare to our data? We will use sim to plot the expected cases per day along with the actual data.

```{r }
sim_out_wide<-rethinking::sim(m.poiss.overall,data=stan_data)

#%>%as_tibble%>%rownames_to_column(var="sample")

ndays <- nrow(stan_data)

sim_out<-as_tibble(sim_out_wide) %>%  
  gather( "index","deaths",1:(ndays)) %>%
  separate(index,c("V","number"),sep=1) %>%
  mutate(number=as.numeric(number)) 
```

```{r }
sim_summarized <- group_by(sim_out,number) %>%
  summarise(deaths_mean=mean(deaths),
            deaths_lower = quantile(deaths,0.05),
            deaths_upper = quantile(deaths,0.95))%>%
  ungroup()
            
```


```{r }
ggplot(sim_summarized, aes(x=number,y=deaths_mean))+
  geom_point()+
  geom_errorbar( aes(ymin=deaths_lower,ymax=deaths_upper))+
  geom_point(data=stan_data, aes(x=index,y=new_deaths), color="red")
```
This model has no inputs into the linear model, it just fits a single expected number of deaths per day. 

We can convert back to the natural scale of the Poisson by exponentiating. We expect about 1.15 deaths per day.
```{r }

tmp<-extract(m.poiss.overall@stanfit) %>% as_tibble()%>%
  mutate(lambda=exp(log_lambda) )
  
bayesplot::mcmc_intervals(tmp,pars=c("lambda"))
```


### Part 1: Poisson model with the week's cases as a predictor
Create a model  where the likelihood model for the daily number of new deaths is Poisson and there is an additive model for the log transformed Poisson parameter. Choose priors based on the range of mean Poisson values that could be produced given the range of the predictor variables. Start with a model where the current week's average Covid cases is the only predictor.


### Part 2:
Once you have samples for your model, explain how you can have some confidence that the chains are doing a good job of approximating the posterior distribution.



### Part 3: 
Now build a series of models with prior week's average case numbers as predictors. 
Compare the WAIC values of the different models. What can you conclude? Do you have any concerns about confounds in your model?




## Bonus du jour: Day of the week effect
There are several issues with when deaths are reported. They are reported when the coroner confirms it as a Covid death, which may be delayed from the actual death. They also may not be reported on some days of the week. Can you test for this effect?
